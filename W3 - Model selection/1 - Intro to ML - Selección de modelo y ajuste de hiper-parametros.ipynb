{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to ML\n",
    "## Selección de modelo y ajuste de hiper-parametros\n",
    "---\n",
    "\n",
    "En la sesión anterior vimos cómo usar la librería *de facto* para machine learning en python: `sklearn`.\n",
    "\n",
    "En la sesión de hoy hablaremos de como las funcionalidades avanzadas de `sklearn` nos facilitan tanto la elección de modelos cómo su validación.\n",
    "\n",
    "**ToC:**\n",
    "\n",
    "1. Selección de Modelos: Encontrando el compromiso entre Bies y Varianza\n",
    "1. Ajuste de hiper-parámetros\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Selección de Modelos\n",
    "---\n",
    "\n",
    "Los algoritmos de ML son herramientas muy potentes que nos permiten tanto ganar información no accesible a primera vista y, además, realizar modelos sobre los datos de forma que podamos hacer predicciones sobre nuevas observaciones.\n",
    "\n",
    "A la hora de encarar un modelado predictivo, tenemos a nuestro haber un abanico muy amplio de modelos sobre los que elegir. La pregunta es: ¿cómo elijo un modelo?\n",
    "\n",
    "### 1.1 Bies y varianza\n",
    "\n",
    "Cómo bien sabéis a estas alturas, si nos dan una serie de atributos (*features*) $X = x_1, ..., x_n$ y un objetivo asociado (*target*) $y = y_1, ...y_n$, existe una función **real** $f(x)$ que nos permite modelar los datos de forma que $y = f(x) + \\epsilon$, dónde $\\epsilon$ es ruido con $\\mu = 0$ y $var = \\sigma^2$\n",
    "\n",
    "Lo que queremos conseguir es un modelo, al que llamaremos $\\hat{f}(x)$ que **aproxime** la función real lo mejor posible. Sucede que independientemente del modelo de ML que seleccionemos para aproximarnos a la función **real** podemos descomponer el **error** que comete nuestro modelo $\\hat{y}$ sobre nuevas observaciones $x_{test}$ en tres componentes distintas:\n",
    "\n",
    "$ Err(x) = \\mathbb{E} \\left[ \\left( y_{test} - \\hat{y} \\right)^2 \\right] = \\left(E[\\hat{f}(x)]-f(x)\\right)^2 + E\\left[\\left(\\hat{f}(x)-E[\\hat{f}(x)]\\right)^2\\right] +\\sigma_e^2$\n",
    "\n",
    "* **Bias (sesgo)**\n",
    "\n",
    "$E[\\hat{f}(x)]-f(x)$\n",
    "\n",
    "La diferencia entre el valor predicho por el modelo y el valor real.\n",
    "\n",
    "Respecto al modelado, puede interpretarse como los supuestos de simplificación que hace un modelo para facilitar el aprendizaje de la función objetivo. El caso típico es el de la regresión lineal: un modelo paramétrico dónde se resuelve un problema de optimización para encontrar el parametro que minimiza el error al hacer predicciones.\n",
    "\n",
    "* **Bajo bies**: Pocos supuestos sobre la forma de los datos, *más flexibles*: Decission trees, knn, ...\n",
    "* **Alto bies**: Simplificaciones sobre la forma de los datos, *menos flexibles*: Linear models\n",
    "\n",
    "\n",
    "* **Varianza**\n",
    "\n",
    "$E\\left[\\left(\\hat{f}(x)-E[\\hat{f}(x)]\\right)^2\\right]$\n",
    "\n",
    "El error producido debido a la sensibilidad del modelo con respecto a los datos de entreno. Imagina que de un set de datos extraemos de forma aleatoria dos subgrupos de entreno. Idealmente, las predicciones del modelo no deberían cambiar mucho de un set de entreno a otro. Esto significaría que el modelo que hemos entrenado está haciendo un buen trabajo haciendo el mapeo de las variables de entrada a y salida.\n",
    "\n",
    "Podemos dividir los distintos algoritmos de ML segun como le afectan los cambios en los datos de entreno:\n",
    "\n",
    "* **Baja Varianza**: Pequeños cambios en las predicciones: Linear models\n",
    "* **Alta Varianza**: Grandes cambios en las prediccinoes: Decission trees, knn, ...\n",
    "\n",
    "### 1.2 Compromiso entre bies y varianza\n",
    "\n",
    "Como hemos visto, los modelos paramétricos, tipo los lineales, son modelos con un alto bies y una baja varianza. Por otro lado, los algoritmos no paramétricos, tipo los arboles de decision, son modelos con bajo bies y alta varianza.\n",
    "\n",
    "![fit_errors](img/underfitting_overfitting.png)\n",
    "\n",
    "Existe un problema de modelado referente a cada uno de los casos. En el primero, corremos el riesgo de so-entreno (underfitting), es decir, que nuestro modelo no sea lo suficientemente flexible como para poder representar la función real que intentamos aproximar. En el segundo caso, nos encontramos en el polo opuesto: nuestro modelo es tan flexible que en vez de aproximarse a la función real se \"aprende\" el set de datos de entreno de memoria, por lo que impide la generalización del modelo fuera del set de entreno.\n",
    "\n",
    "![bies](img/bias-variance.png)\n",
    "\n",
    "Vemos en el gráfico de arriba que un buen modelo es aquel que nos minimiza el error de las predicciones y que este óptimo implica un compromiso entre lo simple o complejo que es mi modelo y los datos que quiero representar.\n",
    "\n",
    "\n",
    "#### Consideraciones prácticas\n",
    "\n",
    "Entonces, ¿cómo prevenimos que nuestro modelo se quede corto o bien no generalice de forma óptima a nuevas observaciones?\n",
    "\n",
    "La única forma de comprobar que nuestro modelo generaliza bien es guardando una parte de los datos para medir el error *out-of-sample*, es decir, el error sobre los datos que no ha visto el modelo durante la fase de entreno.\n",
    "\n",
    "Con tal de poder elegir el mejor modelo y sus parametros se dividirá inicialmente el dataset en dos subsets:\n",
    "\n",
    "* subset de entrenamiento: normalmente se utiliza el 80% del tamaño de la muestra\n",
    "* subset de testeo: el 20% restante.\n",
    "\n",
    "Así mismo, es común y buena práctica utilizar la técnica de **validación cruzada**. Esta técnica se basa en entrenar un modelo con unos hiper-parametros *k*-veces sobre un resampleo aleatorio del subset de entrenamiento en *k-1* y evaluarlo en la restante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación del entorno\n",
    "---\n",
    "**Ejercicio 0: Cómo siempre, empezaremos por cargar las librerías necesarias para poder trabajar con datos en python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import seaborn as ##\n",
    "import matplotlib.pyplot as ##\n",
    "import pandas as ##\n",
    "import numpy as ##\n",
    "\n",
    "## figure aesthetics\n",
    "sns.set_context('talk')\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 1: Carga el dataset de ejemplo por defecto de las casas de boston**\n",
    "\n",
    "scikit-learn viene con una serie de datasets de ejemplo. Carga en una variable `data` el dataset e investiga los atributos de data.\n",
    "\n",
    "\n",
    "data.DESCR --> descripcion del dataset\n",
    "\n",
    "data.feature_names --> lista con el nombre de las columnas\n",
    "\n",
    "data.data -->  array con los datos del dataset (la X)\n",
    "\n",
    "data.target --> array con la variable objetivo (la y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Importa load_boston de sklearn.datasets\n",
    "from sklearn.datasets import ##\n",
    "\n",
    "## Asignalo a `data`\n",
    "data = ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Crea un dataframe a partir de data.data y data.feature_names\n",
    "df = pd.DataFrame(data=##,\n",
    "                  columns=##)\n",
    "\n",
    "## Crea la columna `y` a partir de data.targe\n",
    "## = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 2: Haz un descriptivo rápido y revisa las primeras filas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Realiza un descriptivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Muestra las primeras filas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 3: Haz un plot de la correlación entre variables**\n",
    "\n",
    "Consulta cómo hacerlo aquí --> https://seaborn.pydata.org/examples/many_pairwise_correlations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computa la matriz de correlación\n",
    "corr = ##\n",
    "\n",
    "# Crea una figura de tamaño 12, 12\n",
    "f, ax = plt.subplots(figsize=##)\n",
    "\n",
    "## Plotea el mapa de calor con los valores de la correlacion\n",
    "sns.##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 4: Haz un pairplot para ver somo se relacionan entre ellas con una correlacion > 0.7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtra sobre `corr` aquellos valores que tengan un valor absoluto > 0.7 y < 1\n",
    "highly_corr = corr[(## > 0.7) & (## < 1)]\n",
    "\n",
    "## Haz un `melt` sobre highly corr y elimina los valores nulos\n",
    "\n",
    "highly_corr_melt = ##\n",
    "\n",
    "## Quedate con los valores unicos de la columna variable\n",
    "\n",
    "list_highly_corr_variables = ##\n",
    "\n",
    "print(list_highly_corr_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the pairplot using seaborn\n",
    "\n",
    "sns.pairplot(data=df[##])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Qué conclusiones podemos sacar por ahora ?**\n",
    "\n",
    "**Es un problema de clasificación o regresión?**\n",
    "\n",
    "**Es un problema lineal?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Overfitting y underfitting\n",
    "\n",
    "Para ilustrar de forma práctica el concepto de overfitting y el compromiso entre el bies y la varianza, vamos a simplificar el modelado usando solo la variable `LSTAT` y dos modelos distintos que representen a los dos extremos de los puntos anteriores:\n",
    "\n",
    "* Bajo bies y alta varianza: **Decission tree**\n",
    "\n",
    "* Alto bies y baja varianza: **Linear model**\n",
    "\n",
    "Antes de empezar, vamos a ver de forma clara como luce nuestro problema de una sola variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Nota\n",
    ">\n",
    "> Los arboles de decisión son modelos muy flexibles que no hacen ninguna hipótesis inicial sobre la posible relación entre la variable independiente y los predictores. La forma de \"aprender\" es ir creando splits binarios de forma iterativa sobre las variables. No entraremos en detalle (aún) de cómo funcionan. Bastará con saber para el ejercicio de hoy que uno de los hiperparametros que podemos controlar es la profundidad del arbol, que nos permitirá parar el número de capas de splits que realizamos.\n",
    "\n",
    "\n",
    "\n",
    "![dec_tree](./img/dec_tree.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 5: Haz un grafico con la relación de la variable objetivo `y` con el único predictor, `LSTAT`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['LSTAT', 'y']].plot(kind=##, x=##, y=##)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 6: Divide el dataset en dos sub datasets, el de entreno y el de testeo con el 20% del tamaño total del dataset.**\n",
    "\n",
    "**Haz un grafico como el anterior con los diferentes datasets con colores distintos.**\n",
    "\n",
    "**Comprueba que los datasets de entrenamiento y testeo tienen una distribucion similar**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = ##\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(##, ##, test_size=##, random_state=42)\n",
    "\n",
    "plt.scatter(##, ##, label='train', c='firebrick', alpha=0.5)\n",
    "plt.scatter(##, ##, label='test', alpha=0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comprueba las distribuciones de X e y para train y test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 7: Importa de la libreria sklearn los modelos elegidos anteriormente y entrenalos**\n",
    "\n",
    "DecissionTree vs. LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg_DT = DecisionTreeRegressor(random_state=42)\n",
    "reg_LR = ##\n",
    "\n",
    "reg_DT.fit(##, ##)\n",
    "reg_LR.fit(##, ##)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 8: Evalúa el error producido dentro de la muestra de entreno para los dos algoritmos. Utiliza el RMSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def RMSE(y_true, y_pred):\n",
    "    mse = ##\n",
    "    return np.sqrt(mse)\n",
    "\n",
    "in_sample_error_DT = RMSE(##, ##)\n",
    "in_sample_error_LR = RMSE(##, ##)\n",
    "\n",
    "print('In sample error for DT: {e} miles de $'.format(e=round(in_sample_error_DT, 2)))\n",
    "print('In sample error for LR: {e} miles de $'.format(e= round(in_sample_error_LR, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 9: Evalúa el error producido fuera de la muestra de entreno (datos de test) para los dos algoritmos. Utiliza el RMSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sample_error_DT = RMSE(##, ##)\n",
    "out_sample_error_LR = RMSE(##, ##)\n",
    "\n",
    "print('Out of sample error for DT: {e} miles de $'.format(e=round(out_sample_error_DT, 2)))\n",
    "print('Out of sample error for LR: {e} miles de $'.format(e= round(out_sample_error_LR, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preguntas\n",
    "\n",
    "Qué esta pasando?\n",
    "\n",
    "Parece que un modelo simple de regresión lineal generaliza mejor que un modelo con un bies bajo?\n",
    "\n",
    "Cómo puede ser esto?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 10: Haz una predicción para todos los valores que pueden tomar la variable de predicción** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcula el valor minimo del campo LSTAT\n",
    "x_min = ##\n",
    "\n",
    "# calcula el valor máximo del campo LSTAT\n",
    "x_max = ##\n",
    "\n",
    "# Crea un array de 200 elementos que vayan de manera uniforme entre los límites de LSTAT. (Tip: np.linspace)\n",
    "x = ##\n",
    "\n",
    "# Haz un reshape de los valores usando .reshape(-1, 1). Nos evitará problemas con el resto de la librería.\n",
    "x = x.reshape(-1, 1)\n",
    "\n",
    "# Haz una predicción para los valores del array creado en el paso anterior para cada uno de los modelos\n",
    "y_DT = reg_DT.predict(x)\n",
    "y_LR = reg_LR.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 11: Haz un gráfico x-y con los valores de entreno y test con las predicciones de cada uno de los modelos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crea un subplot de dos columnas con un tamaño de 12 x 6\n",
    "fig, ax = plt.subplots(nrows=##, ncols=##, sharey=True, figsize=(12, 6))\n",
    "\n",
    "# Asigna al primer subplot los datos de entreno contra las predicciones de los dos modelos\n",
    "ax[0].scatter(##, ##, label='train', c='firebrick', alpha=0.5)\n",
    "ax[0].plot(##, ##, '*-', label='DT')\n",
    "ax[0].plot(##, ##, c='orange', label = 'LR')\n",
    "ax[0].legend()\n",
    "\n",
    "# Asigna al segundo subplot los datos de testeo contra las predicciones de los dos modelos\n",
    "ax[1].scatter(##, ##, label='test', c='green', alpha=0.5)\n",
    "ax[1].plot(##, ##, 'X-', label='DT')\n",
    "ax[1].plot(##, ##,  c='orange', label = 'LR')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En efecto, lo que estamos observando aquí en directo es nada más ni nada menos que los efectos de so y sobre entreno. El arbol de decisión, al irse creando a medida que se entrena sobre los datos, se adapta demasiado bien a los datos de entreno y generaliza mal.\n",
    "\n",
    "Uno de los hiper-parametros de los arboles de decisión que nos permite \"jugar\" con la flexibilidad del modelo es la profundidad del arbol (max_depth). Por ahora, no entraremos en detalle en el concepto de \"profundidad\", pero si que lo usaremos de forma programática."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 12: Compara gráficamente como evoluciona el RMSE en función de la profundidad del arbol**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = range(1, 20)\n",
    "in_sample_errors = []\n",
    "out_sample_errors = []\n",
    "for max_depth in max_depths:\n",
    "    tree = DecisionTreeRegressor(max_depth=max_depth).fit(X_train, y_train)\n",
    "    y_pred_train = ##\n",
    "    y_pred_test = ##\n",
    "    in_sample_error = ##\n",
    "    out_sample_error = ##\n",
    "    in_sample_errors.append(##)\n",
    "    out_sample_errors.append(##)\n",
    "\n",
    "## Plot the in sample and out of sample errors vs the depth of the tree\n",
    "plt.plot(##, ##, c='firebrick', label='In-Sample Error')\n",
    "plt.plot(##, ##, label='Out-Sample Error')\n",
    "\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validación cruzada (cross-validation)\n",
    "---\n",
    "Ya hemos visto que el mejor modelo no es aquel que minimiza el error dentro de la muestra de entreno, si no que es aquel que minimiza el error de la muestra de testeo.\n",
    "\n",
    "A pesar de que la división entre datos de entreno y datos de testeo da buenos resultados, es una aproximación básica que no nos permite afinar los hiperparametros de forma correcta. A base de afinar los hiperparametros evaluando el modelo en el set de testeo, podemos acabar incurriendon en una especie de \"filtración\" de la información de los datos de testeo al modelo. Con tal de evitar esto, en vez de dividir sólo en datos de entreno y testeo, deberíamos dividirlos en 3 partes: entreno, validación y test. De esta forma, podemos entrenar y refinar sobre los datos de entreno y validación para que una vez tengamos el modelo \"afinado\" podamos testearlo en el set de test.\n",
    "\n",
    "![train_test](./img/trainvalidationtest.jpg)\n",
    "\n",
    "Esta aproximación, a pesar de ser correcta, implica ciertas limitaciones:\n",
    "\n",
    "1. Reduce drásticamente el volumen de datos con el que estamos trabajando\n",
    "1. No permite hacer \"estadística\" sobre los valores que obtenemos\n",
    "\n",
    "Una forma de hacer frente a estas limitaciónes es mediante la técnica conocida cómo **validación cruzada**. Consiste en dividir el set de datos en $k$ partes iguales. Una vez hecho esto, podemos entrenar el modelo en $k-1$ y testear en aquella que hemos reservado. A esto se le llama *k-fold cross validation*. Entre sus variantes encontramos:\n",
    "\n",
    "- **Stratified K-Fold Validation**: cada sub muestra mantiene la proporción de y's (clasificación)\n",
    "- **Leave-One-Out**: Si $k=n$, dónde $n$ es el número de muestras que tenemos. Intensivo a nivel computacional\n",
    "\n",
    "![kfold](./img/kfold.png)\n",
    "\n",
    "La medidad de rendimiento obtenida mediante validación cruzada es la media de los valores calculados. A pesar de que esta aproximación puede ser computacionalmente cara nos permite aprovechar al máximo el número de muestras sobre las que entrenamos el modelo.\n",
    "\n",
    "Echale un vistazo a la [documentación](http://scikit-learn.org/stable/modules/cross_validation.html)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con tal de ilustrar esta metodología, vamos a utilizar la función implementada en sklearn.\n",
    "\n",
    "Al ser un problema de regresión, optimizaremos los hiperparametros sobre el subset de testeo usando una metrica de rendimiento.\n",
    "\n",
    "También, en sklearn nos permite definir nuestra propia métrica de rendimiento a partir de una funcion. En este caso, usaremos la inversa del RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "rmse_scorer = make_scorer(RMSE, greater_is_better=False)\n",
    "\n",
    "DT_scores = cross_val_score(estimator=##, X=##, y=d##, cv=10, scoring=rmse_scorer)\n",
    "\n",
    "print('RMSE for DT: {m} +- {s}'.format(m=-round(DT_scores.mean(), 2),\n",
    "                                       s=round(DT_scores.std(),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 13: Calcula el RMSE medio para la regresión lineal usando un k=5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_scores = cross_val_score(##)\n",
    "\n",
    "print('RMSE for LR: {m} +- {s}'.format(m=-round(LR_scores.mean(), 2),\n",
    "                                       s=round(LR_scores.std(),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 14: Grafica cómo evoluciona el RMSE medio para k=10, en comparación con los calculados en el ejercicio 12**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializa una lista vacia\n",
    "cv_test_errors_mean = []\n",
    "cv_test_errors_list = []\n",
    "\n",
    "#itera sobre la lista de max_depths del ejercicio 12\n",
    "for max_depth in max_depths:\n",
    "    # Instancia el arbol\n",
    "    tree = ##\n",
    "    \n",
    "    # calcula el error usando cross_val_score y todo el set de datos.\n",
    "    # Recuerda en negar el resultado\n",
    "    cv_test_error = -cross_val_score(##)\n",
    "    #calcula el valor medio y haz un append a la lista\n",
    "    mean_cv_error = ##\n",
    "    cv_test_errors_mean.append(mean_cv_error)\n",
    "    \n",
    "    # haz un append a la lista con todo el array de errores\n",
    "    cv_test_errors_list.append(cv_test_error)\n",
    "\n",
    "plt.plot(max_depths, in_sample_errors, c='firebrick', label='In-Sample Error')\n",
    "plt.plot(max_depths, out_sample_errors, label='Out-Sample Error')\n",
    "plt.plot(max_depths, cv_test_errors_mean, label='Cross-Validation Error')\n",
    "plt.legend(loc='best')\n",
    "        \n",
    "## Graficamos las curvas de aprendizaje de cada subset en el cross_val_score\n",
    "errors = pd.DataFrame(cv_test_errors_list)\n",
    "for e in errors:\n",
    "    plt.plot(max_depths, errors[e], c='gray', alpha=0.2)\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('RMSE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search: ajuste de hiper-parametros.\n",
    "---\n",
    "Como hemos visto en el apartado anterior, podemos ajustar los hiper-parametros de ciertos modelos para aumentar o disminuir su \"flexibilidad\" a la hora de adaptarse a los datos y seleccionar el mejor a partir del resultado de la validación cruzada. Si repitieramos el ejercicio anterior para cada uno de los hiper-parametros del modelo podriamos afinar el modelo para poder exprimirlo completamente.\n",
    "\n",
    "Podemos automátizar este proceso por fuerza bruta definiendo un espacio de busqueda sobre los distintos hiper-parametros que queremos probar y calculando el error cométido para cada una de las combinaciones.\n",
    "\n",
    "Sklearn nos da una herramienta muy útil para tal propósito, GridSearchCV. Esta herramienta evaluará el modelo con cada una de las combinaciones de hiper-parametros utilizando la técnica del cross validation.\n",
    "\n",
    "Veamos como usarla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'max_depth': range(1, 10)}\n",
    "\n",
    "reg = DecisionTreeRegressor()\n",
    "\n",
    "gs_reg_DT = GridSearchCV(estimator=reg,\n",
    "                         param_grid=params,\n",
    "                         cv=5,\n",
    "                         scoring=rmse_scorer)\n",
    "\n",
    "gs_reg_DT.fit(##, ##)\n",
    "print(gs_reg_DT.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 15: Guarda en un dataframe los resultados de las iteraciones. Los encontraras en .cv_results_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results = pd.DataFrame(##)\n",
    "\n",
    "df_cv_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 16: Compara graficamente el mejor modelo DT con la regresion lineal y el arbol de decision iniciales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consideración final del ejercicio\n",
    "\n",
    "En este ejercicio se han tratado los temas de:\n",
    "\n",
    "* Compromiso entre bies-varianza\n",
    "\n",
    "* Como seleccionar un modelo que minimice el error sobre un subset de testeo y/o utilizando validación cruzada\n",
    "\n",
    "* Como realizar una busquedad de hiperparametros utilizando la función de busqueda paramétrica usando validación cruzada\n",
    "\n",
    "> No hay entregable sobre esta práctica. Los contenidos de ésta estarán implicitos en el entregable final."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py37 (ds-uib)",
   "language": "python",
   "name": "ds-uib"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
